# RAG Chatbot API

A context-only Retrieval-Augmented Generation (RAG) chatbot built with FastAPI. It allows users to upload documents, which are ingested into a hybrid retrieval pipeline (BM25 + dense + cross-encoder reranking + HyDE). When a user asks a question, the system retrieves relevant chunks and uses a local LLM to generate grounded responses based solely on those chunks.

---

## Features

- ğŸ“ **Multi-format ingestion**: Supports `.pdf`, `.docx`, `.txt`, `.md`
- ğŸ“š **Hybrid retrieval**: Combines BM25, dense embeddings, HyDE generation, and RRF fusion
- ğŸ§  **Context-only LLM**: Answers strictly from provided documents (no hallucination)
- ğŸ§¾ **Cross-encoder reranking**: Improves retrieval quality
- ğŸ” **Auto reindexing** on document upload/delete
- ğŸ–¼ï¸ **Frontend support**: Serves static HTML/JS/CSS

---

## ğŸ“¦ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/rag-chatbot-api.git
cd rag-chatbot-api

```
### 2. Create a virtual environment 
```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```
if prefer using Conda

```bash
conda create --name <env_name> python=<version>
conda activate <env_name>
```
### 3. Installing dependencies
```bash
pip install -r requirements.txt
```


## Project Structure

.  
â”œâ”€â”€ app.py                       # FastAPI app entrypoint  
â”œâ”€â”€ bulk_ingest.py              # Ingests all files from ./data  
â”œâ”€â”€ data/                       # Uploaded documents  
â”œâ”€â”€ static/                     # Frontend HTML/CSS/JS  
â”œâ”€â”€ models/  
â”‚   â””â”€â”€ llama_wrapper.py        # Local LLM (Mistral/OpenHermes wrapper)  
â”œâ”€â”€ memory/  
â”‚   â”œâ”€â”€ embedder.py             # Embedding logic (MiniLM or similar)  
â”‚   â”œâ”€â”€ memory_store.py         # Vector store (e.g., ChromaDB)  
â”‚   â””â”€â”€ logger.py               # JSON-based chat logger  
â”œâ”€â”€ utils/  
â”‚   â”œâ”€â”€ pipeline_runner.py      # HyDE and standard pipeline runners  
â”‚   â””â”€â”€ prompt_builder.py       # Builds system+context prompts  
â”œâ”€â”€ hybrid_retrieval_pipeline.py# Hybrid RAG retrieval logic  
â”œâ”€â”€ document_ingestor.py        # Ingests individual documents  
â”œâ”€â”€ requirements.txt  
â””â”€â”€ README.md  

## Running the App
```bash
uvicorn app:app --reload
```

### API endpoints
| Method | Endpoint          | Description                      |
| ------ | ----------------- | -------------------------------- |
| GET    | `/`               | Serves the frontend UI           |
| POST   | `/upload`         | Uploads and ingests a document   |
| POST   | `/chat`           | Chat with the LLM using RAG      |
| GET    | `/documents`      | Lists all uploaded documents     |
| DELETE | `/documents/{fn}` | Deletes a document and reindexes |
| POST   | `/reindex`        | Manually reindex all files       |
| GET    | `/health`         | Health check for readiness       |


### LLM + Retrieval

    LLM: openhermes-2.5-mistral-7b.Q4_K_M.gguf (run locally via llama-cpp-python)

    Embedding: all-MiniLM-L6-v2 via SentenceTransformers

    Cross-Encoder: ms-marco-MiniLM-L-6-v2

    Vector store: ChromaDB (can be swapped)


### Sample screenshots
![Screenshot 2025-06-06 140004](https://github.com/user-attachments/assets/9f0decb4-c6f9-4f65-9b21-a52ca16f3eed)

